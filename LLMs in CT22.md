


<h3>LLMs in CT22 dataset</h3>
<table>
  <thead>
    <tr>
      <th>Partition</th><th>Model</th>
      <th colspan="4">Uncleaned Text</th><th colspan="4">Cleaned Text</th>
    </tr>
    <tr>
      <th></th><th></th>
      <th>Acc.</th><th>Prec.</th><th>F1</th><th>Rec.</th>
      <th>Acc.</th><th>Prec.</th><th>F1</th><th>Rec.</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Test</td><td>Llama-2-7b</td>
      <td style="background:#b2f0b2;">⭐0.8153</td><td style="background:#b2f0b2;">⭐0.5571</td><td style="background:#b2f0b2;"><strong>⭐0.6003</strong></td><td style="background:#b2f0b2;">⭐0.6393</td>
      <td>0.8054</td><td>0.6667</td><td style="background:#d3d3d3;"><strong>0.5797</strong></td><td>0.5128</td>
    </tr>
    <tr>
      <td></td><td>Llama-3.2-1b</td>
      <td>0.6309</td><td>0.3462</td><td><strong>0.3956</strong></td><td>0.4615</td>
      <td>0.5928</td><td>0.3537</td><td><strong>0.4621</strong></td><td style="background:#b2f0b2;">0.6667</td>
    </tr>
    <tr>
      <td></td><td>GPT-2</td>
      <td style="background:#b2f0b2;">0.6465</td><td style="background:#b2f0b2;">0.2589</td><td style="background:#b2f0b2;"><strong>0.2176</strong></td><td style="background:#b2f0b2;">0.1880</td>
     <td>0.6395</td><td>0.2819</td><td style="background:#d3d3d3;"><strong>0.1594</strong></td><td>0.1111</td>
    </tr>
    <tr>
       <tr>
      <td></td><td>TinyLlama</td>
      <td style="background:#b2f0b2;">0.6309</td><td style="background:#b2f0b2;">0.2647</td><td style="background:#b2f0b2;"><strong>0.2466</strong></td><td style="background:#b2f0b2;">0.2308</td>
      <td style="background:#b2f0b2;"></td><td style="background:#b2f0b2;"></td><td></td><td></td>
    </tr>
    <tr>
      <td>Dev-Test</td><td>Llama-2-7b</td>
      <td style="background:#b2f0b2;">⭐0.7944</td><td style="background:#b2f0b2;">⭐0.5652</td><td style="background:#b2f0b2;"><strong>⭐0.5693</strong></td><td style="background:#b2f0b2;">⭐0.5735</td>
      <td style="background:#b2f0b2;">0.8066</td><td style="background:#b2f0b2;">0.7045</td><td><strong>0.3584</strong></td><td>0.2403</td>
    </tr>
    <tr>
      <td></td><td>Llama-3.2-1b</td>
      <td>0.7753</td><td>0.5</td><td style="background:#d3d3d3;"><strong>0.4267</strong></td><td>0.3721</td>
      <td>0.7631</td><td>0.4791</td><td style="background:#d3d3d3;"><strong>0.5416</strong></td><td style="background:#b2f0b2;">0.6227</td>
    </tr>
    <tr>
      <td></td><td>GPT-2</td>
      <td>0.7718</td><td>0.4720</td><td style="background:#d3d3d3;"><strong>0.1997</strong></td><td>0.1266</td>
      <td></td><td></td><td></td><td style="background:#b2f0b2;"></td>
    </tr>
    <tr>
      <td></td><td>TinyLlama</td>
      <td style="background:#b2f0b2;">0.7619</td><td style="background:#b2f0b2;">0.4112</td><td style="background:#b2f0b2;"><strong>0.2053</strong></td><td style="background:#b2f0b2;">0.1370</td>
      <td style="background:#b2f0b2;"></td><td style="background:#b2f0b2;"></td><td></td><td></td>
    </tr>

    
  </tbody>
</table>

<p><strong>Table 8:</strong> Performance of LLMs in CT22 dataset in test and dev-test set.</p>


