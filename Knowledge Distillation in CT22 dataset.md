### Performance  of LLama2-7b-hf
| Dataset   | Accuracy | Precision | F1 | Recall |                                                              
|-----------|----------|----------|-----------|--------|              
| Test  |  |     | ****    |   |              
|  Dev-Test  |   |   | ****   |    | 

### Distillation Setup
- Teacher: `LLaMA2-7B-hf`
- Student: `TinyLLaMA-Chat`

### Student Evaluation Results                                        

| Dataset   | Accuracy | Precision | F1 | Recall |                                                              
|-----------|----------|----------|-----------|--------|              
| Test  |  |     | ****    |   |              
|  Dev-Test  |    |    | ****   |   | 

### performance of TinyLLaMa before knowledge distillation

| Dataset   | Accuracy | Precision | F1 | Recall |                                                              
|-----------|----------|----------|-----------|--------|              
| Test  |  |    | ****    |   |              
|  Dev-Test  |   |    | ****   |   | 



### Distillation Setup
- Teacher: `LLaMA2-7B-hf`
- Student: `microsoft/phi-2`

### Student Evaluation Results

| Dataset   | Accuracy | Precision | F1 | Recall |
|-----------|----------|----------|-----------|--------|
| Test  |  |    | ****    |   |
|  Dev-Test  |    |    | ****   |   |


### performance of phi2 before Knowledge distillation

| Dataset   | Accuracy | Precision | F1 | Recall |
|-----------|----------|----------|-----------|--------|
| Test  |  |    | ****    |   |
|  Dev-Test  |    |    | ****   |   |
